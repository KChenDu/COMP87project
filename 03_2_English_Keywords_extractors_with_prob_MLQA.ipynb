{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLQA Extract `English` Keywords (Query/Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 23:06:13.986858: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-21 23:06:13.986980: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-21 23:06:14.143813: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForTokenClassification\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 9916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'title': 'Area 51',\n",
       "  'context': 'In 1994, five unnamed civilian contractors and the widows of contractors Walter Kasza and Robert Frost sued the USAF and the United States Environmental Protection Agency. Their suit, in which they were represented by George Washington University law professor Jonathan Turley, alleged they had been present when large quantities of unknown chemicals had been burned in open pits and trenches at Groom. Biopsies taken from the complainants were analyzed by Rutgers University biochemists, who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat. The complainants alleged they had sustained skin, liver, and respiratory injuries due to their work at Groom, and that this had contributed to the deaths of Frost and Kasza. The suit sought compensation for the injuries they had sustained, claiming the USAF had illegally handled toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs handling of dangerous materials). They also sought detailed information about the chemicals to which they were allegedly exposed, hoping this would facilitate the medical treatment of survivors. Congressman Lee H. Hamilton, former chairman of the House Intelligence Committee, told 60 Minutes reporter Lesley Stahl, \"The Air Force is classifying all information about Area 51 in order to protect themselves from a lawsuit.\"'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path = '/kaggle/input/m-l-q-a/Context_EN.json'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(data)}\")\n",
    "data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART method (Query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up BART**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20913754d2424e199120ffa477e578a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91beaab1738e4a41b1275fef62c7678d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52d98adda124d1d8bf5c9845bb0ded7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce86b4c5281c400ca1986caf1297789d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d90ea905c24f1da40eadeacbb7a451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568b83399a1f4345b26aa2bfd7ae72b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2517560ae9411592436e3396cf0f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ace40e845d4787becb0ea611aa866b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/292 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Andyrasika/bart_tech_keywords\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Andyrasika/bart_tech_keywords\")\n",
    "\n",
    "'''\n",
    "    * For \"Question\", set `texts` with `[item['question'] for item in data]`\n",
    "    * For \"Context\", set `texts` with `[item['context'] for item in data]`\n",
    "'''\n",
    "texts = [item['context'] for item in data]\n",
    "\n",
    "encodings = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Use dataloader\n",
    "dataset = TextDataset(encodings)\n",
    "dataloader = DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670eb2a8bcba431fa5074e2be8ee292f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating text:   0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34/2267505483.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1128: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "text_outputs = []\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Generating text\"):\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()} \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**batch)\n",
    "        decoded_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        text_outputs.extend(decoded_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9916, 9916)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert len(texts) == len(text_outputs)\n",
    "len(texts), len(text_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 9916\n"
     ]
    }
   ],
   "source": [
    "strip_key_words = [line.strip() for line in text_outputs]\n",
    "\n",
    "'''\n",
    "    * For \"Question\", set `ids` with `item['id']`\n",
    "    * For \"Context\", set `ids` with `item['context_id']`\n",
    "'''\n",
    "ids = [item['id'] for item in data]\n",
    "titles = [item['title'] for item in data]\n",
    "\n",
    "assert len(ids) == len(strip_key_words)\n",
    "assert len(ids) == len(titles)\n",
    "\n",
    "print(f\"Total num: {len(strip_key_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"id\": id, \n",
    "        \"title\": title,\n",
    "        \"keywords\": [k.strip() for k in kw.split(',') if k.strip()]  # Strip and remove empty values\n",
    "    } \n",
    "    for id, title, kw in zip(ids, titles, strip_key_words)\n",
    "]\n",
    "\n",
    "json_data = json.dumps(documents, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/kaggle/working/Context_BART_Keywords_MLQA.json', 'w') as f:\n",
    "    f.write(json_data)\n",
    "\n",
    "# with open('./data/BART_Keywords.json', 'w') as f:\n",
    "#     f.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial add prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 11590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'a4968ca8a18de16aa3859be760e43dbd3af3fce9',\n",
       "  'keywords': ['Biopsies', 'Analysis']},\n",
       " {'id': 'f251ea56c4f1aa1df270137f7e6d89c0cc1b6ef4',\n",
       "  'keywords': ['Robert Frost', 'Walter Kasza', 'Lawsuit']},\n",
       " {'id': '04ecd5555635bc05fd2f379d1b9027edd663cebf',\n",
       "  'keywords': ['Lawsuit', 'Groom']},\n",
       " {'id': 'd066a75dbe8cd3e2b57c415a8eb54a08dc7e72a7',\n",
       "  'keywords': ['Complaints', 'Allegations']},\n",
       " {'id': 'c5f545baccd8ea8adb83f75756f4832340600bd9',\n",
       "  'keywords': ['Aerospace Magazine']}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path_ = '/kaggle/working/Query_BART_Keywords_MLQA.json'\n",
    "\n",
    "with open(file_path_, 'r') as f:\n",
    "    Query_BART_Keywords = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(Query_BART_Keywords)}\")\n",
    "Query_BART_Keywords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'a4968ca8a18de16aa3859be760e43dbd3af3fce9',\n",
       "  'keywords': ['Biopsies', 'Analysis'],\n",
       "  'prob': [0.9, 0.8]},\n",
       " {'id': 'f251ea56c4f1aa1df270137f7e6d89c0cc1b6ef4',\n",
       "  'keywords': ['Robert Frost', 'Walter Kasza', 'Lawsuit'],\n",
       "  'prob': [0.9, 0.8, 0.7]},\n",
       " {'id': '04ecd5555635bc05fd2f379d1b9027edd663cebf',\n",
       "  'keywords': ['Lawsuit', 'Groom'],\n",
       "  'prob': [0.9, 0.8]},\n",
       " {'id': 'd066a75dbe8cd3e2b57c415a8eb54a08dc7e72a7',\n",
       "  'keywords': ['Complaints', 'Allegations'],\n",
       "  'prob': [0.9, 0.8]},\n",
       " {'id': 'c5f545baccd8ea8adb83f75756f4832340600bd9',\n",
       "  'keywords': ['Aerospace Magazine'],\n",
       "  'prob': [0.9]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in Query_BART_Keywords:\n",
    "    num_keywords = len(item[\"keywords\"])\n",
    "    probs = [max(0.9 - 0.1 * i, 0.1) for i in range(num_keywords)]\n",
    "    item[\"prob\"] = probs\n",
    "\n",
    "Query_BART_Keywords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.dumps(Query_BART_Keywords, indent=4)\n",
    "\n",
    "with open('/kaggle/working/Query_BART_Keywords_artificially_prob_MLQA.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bart generate prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set model to evaluate mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13367d2820f5418e818488842f11e0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34/4251668379.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "all_input_texts = []\n",
    "all_generated_texts = []\n",
    "all_probabilities = []\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    input_ids = batch['input_ids'].to(model.device)\n",
    "    attention_mask = batch['attention_mask'].to(model.device)\n",
    "\n",
    "    for idx in range(input_ids.size(0)):  # Iter all in a batch\n",
    "        decoded_ids = torch.full((1, 1), tokenizer.bos_token_id, dtype=torch.long, device=model.device)\n",
    "        seq_probabilities = []\n",
    "\n",
    "        input_text = tokenizer.decode(input_ids[idx].tolist(), skip_special_tokens=True)\n",
    "        all_input_texts.append(input_text) \n",
    "        \n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids[idx:idx+1], attention_mask=attention_mask[idx:idx+1], decoder_input_ids=decoded_ids)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "                probs = softmax(logits, dim=-1)\n",
    "                next_token_id = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "                next_token_prob = probs[0, next_token_id.item()].item()\n",
    "\n",
    "                if next_token_id == tokenizer.eos_token_id or decoded_ids.size(1) >= 30:\n",
    "                    break\n",
    "\n",
    "                decoded_ids = torch.cat([decoded_ids, next_token_id], dim=-1)\n",
    "                seq_probabilities.append(next_token_prob)\n",
    "\n",
    "        generated_text = tokenizer.decode(decoded_ids[:, 1:].squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "        # Splitting the generated text and probability\n",
    "        segments = generated_text.split(',')\n",
    "        segment_probs = []\n",
    "        start_idx = 0\n",
    "        for segment in segments:\n",
    "            segment_tokens = tokenizer.tokenize(segment.strip())\n",
    "            segment_length = len(segment_tokens)\n",
    "            # Calc avg prob of the segment\n",
    "            if segment_length > 0:\n",
    "                segment_prob = sum(seq_probabilities[start_idx:start_idx+segment_length]) / segment_length\n",
    "                segment_probs.append(f\"{segment_prob:.6f}\")\n",
    "            start_idx += segment_length\n",
    "\n",
    "        all_generated_texts.append(generated_text)\n",
    "        all_probabilities.append(\",\".join(segment_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_key_words = [line.strip() for line in all_generated_texts]\n",
    "\n",
    "'''\n",
    "    * For \"Question\", set `ids` with `[item['id'] for item in data]`\n",
    "    * For \"Context\", set `ids` with `[item['context_id'] for item in data]`\n",
    "'''\n",
    "ids = [item['id'] for item in data]\n",
    "\n",
    "len(ids),len(strip_key_words)\n",
    "\n",
    "# print(f\"Total num: {len(strip_key_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = [\n",
    "#     {\"context_id\": id, \"keywords\": [k.strip() for k in kw.split(',')], \"prob\": [k.strip() for k in p.split(',')]} \n",
    "#     for id, kw, p in zip(ids, strip_key_words, all_probabilities)\n",
    "# ]\n",
    "\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": id, \n",
    "        \"keywords\": [k.strip() for k in kw.split(',')],\n",
    "        \"prob\": [float(p) for p in prob.split(',')]\n",
    "    } \n",
    "    for id, kw, prob in zip(ids, strip_key_words, all_probabilities)\n",
    "    if kw.strip() != '' and prob.strip() != ''\n",
    "]\n",
    "\n",
    "json_data = json.dumps(documents, indent=4)\n",
    "# print(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/kaggle/working/Query_BART_Keywords_with_prob_MLQA.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_ = '/kaggle/working/Query_BART_Keywords_with_prob_MLQA.json'\n",
    "\n",
    "with open(file_path_, 'r') as f:\n",
    "    data_ = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(data_)}\")\n",
    "data_[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
