{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLQA Extract `English` Keywords (Query/Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForTokenClassification\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 9916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'title': 'Area 51',\n",
       "  'context': 'In 1994, five unnamed civilian contractors and the widows of contractors Walter Kasza and Robert Frost sued the USAF and the United States Environmental Protection Agency. Their suit, in which they were represented by George Washington University law professor Jonathan Turley, alleged they had been present when large quantities of unknown chemicals had been burned in open pits and trenches at Groom. Biopsies taken from the complainants were analyzed by Rutgers University biochemists, who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat. The complainants alleged they had sustained skin, liver, and respiratory injuries due to their work at Groom, and that this had contributed to the deaths of Frost and Kasza. The suit sought compensation for the injuries they had sustained, claiming the USAF had illegally handled toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs handling of dangerous materials). They also sought detailed information about the chemicals to which they were allegedly exposed, hoping this would facilitate the medical treatment of survivors. Congressman Lee H. Hamilton, former chairman of the House Intelligence Committee, told 60 Minutes reporter Lesley Stahl, \"The Air Force is classifying all information about Area 51 in order to protect themselves from a lawsuit.\"'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path = '/kaggle/input/m-l-q-a/Context_EN.json'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(data)}\")\n",
    "data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART method (Query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up BART**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20913754d2424e199120ffa477e578a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91beaab1738e4a41b1275fef62c7678d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52d98adda124d1d8bf5c9845bb0ded7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce86b4c5281c400ca1986caf1297789d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d90ea905c24f1da40eadeacbb7a451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568b83399a1f4345b26aa2bfd7ae72b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2517560ae9411592436e3396cf0f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ace40e845d4787becb0ea611aa866b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/292 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Andyrasika/bart_tech_keywords\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Andyrasika/bart_tech_keywords\")\n",
    "\n",
    "'''\n",
    "    * For \"Question\", set `texts` with `[item['question'] for item in data]`\n",
    "    * For \"Context\", set `texts` with `[item['context'] for item in data]`\n",
    "'''\n",
    "texts = [item['context'] for item in data]\n",
    "\n",
    "encodings = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Use dataloader\n",
    "dataset = TextDataset(encodings)\n",
    "dataloader = DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670eb2a8bcba431fa5074e2be8ee292f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating text:   0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34/2267505483.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1128: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "text_outputs = []\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Generating text\"):\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()} \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**batch)\n",
    "        decoded_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        text_outputs.extend(decoded_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9916, 9916)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assert len(texts) == len(text_outputs)\n",
    "len(texts), len(text_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 9916\n"
     ]
    }
   ],
   "source": [
    "strip_key_words = [line.strip() for line in text_outputs]\n",
    "\n",
    "'''\n",
    "    * For \"Question\", set `ids` with `item['id']`\n",
    "    * For \"Context\", set `ids` with `item['context_id']`\n",
    "'''\n",
    "ids = [item['id'] for item in data]\n",
    "titles = [item['title'] for item in data]\n",
    "\n",
    "assert len(ids) == len(strip_key_words)\n",
    "assert len(ids) == len(titles)\n",
    "\n",
    "print(f\"Total num: {len(strip_key_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"id\": id, \n",
    "        \"title\": title,\n",
    "        \"keywords\": [k.strip() for k in kw.split(',') if k.strip()]  # Strip and remove empty values\n",
    "    } \n",
    "    for id, title, kw in zip(ids, titles, strip_key_words)\n",
    "]\n",
    "\n",
    "json_data = json.dumps(documents, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/kaggle/working/Context_BART_Keywords_MLQA_en.json', 'w') as f:\n",
    "    f.write(json_data)\n",
    "\n",
    "# with open('./data/BART_Keywords.json', 'w') as f:\n",
    "#     f.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial add prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 11590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'a4968ca8a18de16aa3859be760e43dbd3af3fce9',\n",
       "  'keywords': ['Biopsies', 'Analysis']},\n",
       " {'id': 'f251ea56c4f1aa1df270137f7e6d89c0cc1b6ef4',\n",
       "  'keywords': ['Robert Frost', 'Walter Kasza', 'Lawsuit']},\n",
       " {'id': '04ecd5555635bc05fd2f379d1b9027edd663cebf',\n",
       "  'keywords': ['Lawsuit', 'Groom']},\n",
       " {'id': 'd066a75dbe8cd3e2b57c415a8eb54a08dc7e72a7',\n",
       "  'keywords': ['Complaints', 'Allegations']},\n",
       " {'id': 'c5f545baccd8ea8adb83f75756f4832340600bd9',\n",
       "  'keywords': ['Aerospace Magazine']}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path_ = '/kaggle/working/Query_BART_Keywords_MLQA_en.json'\n",
    "\n",
    "with open(file_path_, 'r') as f:\n",
    "    Query_BART_Keywords = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(Query_BART_Keywords)}\")\n",
    "Query_BART_Keywords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'a4968ca8a18de16aa3859be760e43dbd3af3fce9',\n",
       "  'keywords': ['Biopsies', 'Analysis'],\n",
       "  'prob': [0.9, 0.8]},\n",
       " {'id': 'f251ea56c4f1aa1df270137f7e6d89c0cc1b6ef4',\n",
       "  'keywords': ['Robert Frost', 'Walter Kasza', 'Lawsuit'],\n",
       "  'prob': [0.9, 0.8, 0.7]},\n",
       " {'id': '04ecd5555635bc05fd2f379d1b9027edd663cebf',\n",
       "  'keywords': ['Lawsuit', 'Groom'],\n",
       "  'prob': [0.9, 0.8]},\n",
       " {'id': 'd066a75dbe8cd3e2b57c415a8eb54a08dc7e72a7',\n",
       "  'keywords': ['Complaints', 'Allegations'],\n",
       "  'prob': [0.9, 0.8]},\n",
       " {'id': 'c5f545baccd8ea8adb83f75756f4832340600bd9',\n",
       "  'keywords': ['Aerospace Magazine'],\n",
       "  'prob': [0.9]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in Query_BART_Keywords:\n",
    "    num_keywords = len(item[\"keywords\"])\n",
    "    probs = [max(0.9 - 0.1 * i, 0.1) for i in range(num_keywords)]\n",
    "    item[\"prob\"] = probs\n",
    "\n",
    "Query_BART_Keywords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.dumps(Query_BART_Keywords, indent=4)\n",
    "\n",
    "with open('/kaggle/working/Query_BART_Keywords_artificially_prob_MLQA_en.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Add `title` to `query`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **For `Query_BART_Keywords_MLQA_en.json`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 11590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'Who analyzed the biopsies?',\n",
       "  'answers': [{'text': 'Rutgers University biochemists', 'answer_start': 457}],\n",
       "  'id': 'a4968ca8a18de16aa3859be760e43dbd3af3fce9',\n",
       "  'context_id': 0,\n",
       "  'title': 'Area 51'},\n",
       " {'question': 'who represented robert frost and walter kasza in their suit?',\n",
       "  'answers': [{'text': 'George Washington University law professor Jonathan Turley',\n",
       "    'answer_start': 218}],\n",
       "  'id': 'f251ea56c4f1aa1df270137f7e6d89c0cc1b6ef4',\n",
       "  'context_id': 0,\n",
       "  'title': 'Area 51'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path = './data/QA_EN.json'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    data_en_qa = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(data_en_qa)}\")\n",
    "data_en_qa[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 11590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'a4968ca8a18de16aa3859be760e43dbd3af3fce9',\n",
       "  'keywords': ['Biopsies', 'Analysis']},\n",
       " {'id': 'f251ea56c4f1aa1df270137f7e6d89c0cc1b6ef4',\n",
       "  'keywords': ['Robert Frost', 'Walter Kasza', 'Lawsuit']}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path = './data/MLQA_Keywords/en/Query_BART_Keywords_MLQA_en.json'\n",
    "# file_path = './data/MLQA_Keywords/en/Query_BART_Keywords_artificially_prob_MLQA_en.json'\n",
    "# file_path = './data/MLQA_Keywords/en/Query_BART_Keywords_with_prob_MLQA_en.json'\n",
    "\n",
    "# file_path = './data/MLQA_Keywords/en/Context_with_title_BART_Keywords_MLQA_en.json'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    query_kw = json.load(f)\n",
    "print(f\"Total num: {len(query_kw)}\")\n",
    "query_kw[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 11590\n"
     ]
    }
   ],
   "source": [
    "ids = [item['id'] for item in data_en_qa]\n",
    "context_ids = [item['context_id'] for item in data_en_qa]\n",
    "titles = [item['title'] for item in data_en_qa]\n",
    "# Clean title\n",
    "# titles = [re.sub(r'\\W+', '', title) for title in titles]\n",
    "titles = [re.sub(r'[^\\w\\s]+', '', title) for title in titles]\n",
    "\n",
    "keywords_list = [item['keywords'] for item in query_kw]\n",
    "# prob_list = [item['prob'] for item in query_kw]\n",
    "\n",
    "assert len(ids_list) == len(keywords_list)\n",
    "# assert len(ids_list) == len(prob_list)\n",
    "\n",
    "\n",
    "strip_key_words = [([titles[i]] + sublist) for i, sublist in enumerate(keywords_list)]\n",
    "\n",
    "assert len(ids) == len(strip_key_words)\n",
    "\n",
    "print(f\"Total num: {len(strip_key_words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop duplicate title and keywords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 11590\n"
     ]
    }
   ],
   "source": [
    "unique_data = [list(dict.fromkeys(sublist)) for sublist in strip_key_words]\n",
    "\n",
    "assert len(unique_data) == len(strip_key_words)\n",
    "print(f\"Total num: {len(unique_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {   \n",
    "        \"id\": id, \n",
    "        \"context id\": c_id,\n",
    "        \"keywords\": kw\n",
    "    } \n",
    "    for id, c_id, kw in zip(ids, context_ids, unique_data)\n",
    "]\n",
    "\n",
    "json_data = json.dumps(documents, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/MLQA_Keywords/en/Query_BART_Keywords_with_title_MLQA_en.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'a4968ca8a18de16aa3859be760e43dbd3af3fce9',\n",
       "  'context id': 0,\n",
       "  'keywords': ['Area 51', 'Biopsies', 'Analysis']},\n",
       " {'id': 'f251ea56c4f1aa1df270137f7e6d89c0cc1b6ef4',\n",
       "  'context id': 0,\n",
       "  'keywords': ['Area 51', 'Robert Frost', 'Walter Kasza', 'Lawsuit']}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path = './data/MLQA_Keywords/en/Query_BART_Keywords_with_title_MLQA_en.json'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    Query_BART_Keywords = json.load(f)\n",
    "Query_BART_Keywords[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **For `Query_BART_Keywords_artificially_prob_MLQA_en.json`**\n",
    "\n",
    "**Add prob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'a4968ca8a18de16aa3859be760e43dbd3af3fce9',\n",
       "  'context id': 0,\n",
       "  'keywords': ['Area 51', 'Biopsies', 'Analysis'],\n",
       "  'prob': [0.9, 0.8, 0.7]},\n",
       " {'id': 'f251ea56c4f1aa1df270137f7e6d89c0cc1b6ef4',\n",
       "  'context id': 0,\n",
       "  'keywords': ['Area 51', 'Robert Frost', 'Walter Kasza', 'Lawsuit'],\n",
       "  'prob': [0.9, 0.8, 0.7, 0.6]},\n",
       " {'id': '04ecd5555635bc05fd2f379d1b9027edd663cebf',\n",
       "  'context id': 0,\n",
       "  'keywords': ['Area 51', 'Lawsuit', 'Groom'],\n",
       "  'prob': [0.9, 0.8, 0.7]},\n",
       " {'id': 'd066a75dbe8cd3e2b57c415a8eb54a08dc7e72a7',\n",
       "  'context id': 0,\n",
       "  'keywords': ['Area 51', 'Complaints', 'Allegations'],\n",
       "  'prob': [0.9, 0.8, 0.7]},\n",
       " {'id': 'c5f545baccd8ea8adb83f75756f4832340600bd9',\n",
       "  'context id': 1,\n",
       "  'keywords': ['Area 51', 'Aerospace Magazine'],\n",
       "  'prob': [0.9, 0.8]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in Query_BART_Keywords:\n",
    "    num_keywords = len(item[\"keywords\"])\n",
    "    probs = [max(0.9 - 0.1 * i, 0.1) for i in range(num_keywords)]\n",
    "    item[\"prob\"] = probs\n",
    "\n",
    "Query_BART_Keywords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.dumps(Query_BART_Keywords, indent=4)\n",
    "\n",
    "with open('./data/MLQA_Keywords/en/Query_BART_Keywords_with_artificially_prob_MLQA_en.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bart generate prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set model to evaluate mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42b850268364e179b0d0a3fa08dd3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34/4251668379.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "all_input_texts = []\n",
    "all_generated_texts = []\n",
    "all_probabilities = []\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    input_ids = batch['input_ids'].to(model.device)\n",
    "    attention_mask = batch['attention_mask'].to(model.device)\n",
    "\n",
    "    for idx in range(input_ids.size(0)):  # Iter all in a batch\n",
    "        decoded_ids = torch.full((1, 1), tokenizer.bos_token_id, dtype=torch.long, device=model.device)\n",
    "        seq_probabilities = []\n",
    "\n",
    "        input_text = tokenizer.decode(input_ids[idx].tolist(), skip_special_tokens=True)\n",
    "        all_input_texts.append(input_text) \n",
    "        \n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids[idx:idx+1], attention_mask=attention_mask[idx:idx+1], decoder_input_ids=decoded_ids)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "                probs = softmax(logits, dim=-1)\n",
    "                next_token_id = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "                next_token_prob = probs[0, next_token_id.item()].item()\n",
    "\n",
    "                if next_token_id == tokenizer.eos_token_id or decoded_ids.size(1) >= 30:\n",
    "                    break\n",
    "\n",
    "                decoded_ids = torch.cat([decoded_ids, next_token_id], dim=-1)\n",
    "                seq_probabilities.append(next_token_prob)\n",
    "\n",
    "        generated_text = tokenizer.decode(decoded_ids[:, 1:].squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "        # Splitting the generated text and probability\n",
    "        segments = generated_text.split(',')\n",
    "        segment_probs = []\n",
    "        start_idx = 0\n",
    "        for segment in segments:\n",
    "            segment_tokens = tokenizer.tokenize(segment.strip())\n",
    "            segment_length = len(segment_tokens)\n",
    "            # Calc avg prob of the segment\n",
    "            if segment_length > 0:\n",
    "                segment_prob = sum(seq_probabilities[start_idx:start_idx+segment_length]) / segment_length\n",
    "                segment_probs.append(f\"{segment_prob:.6f}\")\n",
    "            start_idx += segment_length\n",
    "\n",
    "        all_generated_texts.append(generated_text)\n",
    "        all_probabilities.append(\",\".join(segment_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11590, 11590)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(all_generated_texts), len(all_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 11590\n"
     ]
    }
   ],
   "source": [
    "strip_key_words = [line.strip() for line in all_generated_texts]\n",
    "\n",
    "'''\n",
    "    * For \"Question\", set `ids` with `[item['id'] for item in data]`\n",
    "    * For \"Context\", set `ids` with `[item['context_id'] for item in data]`\n",
    "'''\n",
    "\n",
    "ids = [item['id'] for item in data_en_qa]\n",
    "context_ids = [item['context_id'] for item in data_en_qa]\n",
    "\n",
    "len(ids),len(strip_key_words)\n",
    "\n",
    "print(f\"Total num: {len(strip_key_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output `WITHOUT title` version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"id\": id, \n",
    "        \"context id\": c_id,\n",
    "        \"keywords\": [k.strip() for k in kw.split(',')],\n",
    "        \"prob\": [float(p) for p in prob.split(',')]\n",
    "    } \n",
    "    for id, c_id, kw, prob in zip(ids, context_ids, strip_key_words, all_probabilities)\n",
    "    if kw.strip() != '' and prob.strip() != ''\n",
    "]\n",
    "\n",
    "json_data = json.dumps(documents, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/kaggle/working/Query_BART_Keywords_with_prob_MLQA_en.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 11110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'a4968ca8a18de16aa3859be760e43dbd3af3fce9',\n",
       "  'context id': 0,\n",
       "  'keywords': ['Bi', 'Biopsies'],\n",
       "  'prob': [0.80947, 0.553737]},\n",
       " {'id': 'f251ea56c4f1aa1df270137f7e6d89c0cc1b6ef4',\n",
       "  'context id': 0,\n",
       "  'keywords': ['RobertKasza', 'Frost', 'Lawsuit'],\n",
       "  'prob': [0.673948, 0.663587, 0.753613]},\n",
       " {'id': '04ecd5555635bc05fd2f379d1b9027edd663cebf',\n",
       "  'context id': 0,\n",
       "  'keywords': ['Groom', 'Lawsuit'],\n",
       "  'prob': [0.424797, 0.579578]},\n",
       " {'id': 'd066a75dbe8cd3e2b57c415a8eb54a08dc7e72a7',\n",
       "  'context id': 0,\n",
       "  'keywords': ['Complaintants', 'alleged'],\n",
       "  'prob': [0.639551, 0.296297]},\n",
       " {'id': 'c5f545baccd8ea8adb83f75756f4832340600bd9',\n",
       "  'context id': 1,\n",
       "  'keywords': ['A Aerospace Magazine'],\n",
       "  'prob': [0.599785]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path_ = '/kaggle/working/Query_BART_Keywords_with_prob_MLQA_en.json'\n",
    "\n",
    "with open(file_path_, 'r') as f:\n",
    "    data_ = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(data_)}\")\n",
    "data_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output `with title` version**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 11590\n"
     ]
    }
   ],
   "source": [
    "ids = [item['id'] for item in data_en_qa]\n",
    "context_ids = [item['context_id'] for item in data_en_qa]\n",
    "titles = [item['title'] for item in data_en_qa]\n",
    "# Clean title\n",
    "# titles = [re.sub(r'\\W+', '', title) for title in titles]\n",
    "titles = [re.sub(r'[^\\w\\s]+', '', title) for title in titles]\n",
    "\n",
    "keywords_list = [item.split(', ') for item in strip_key_words]\n",
    "prob_list = [\n",
    "    [float(num) if num != '' else 0.0 for num in item.split(',')] \n",
    "    for item in all_probabilities\n",
    "]\n",
    "\n",
    "assert len(ids) == len(keywords_list)\n",
    "assert len(ids) == len(prob_list)\n",
    "\n",
    "updated_keywords_list = []\n",
    "updated_prob_list = []\n",
    "\n",
    "for i, (sublist, prob_sublist) in enumerate(zip(keywords_list, prob_list)):\n",
    "    # If the keywords sublist is empty\n",
    "    if not sublist:  \n",
    "        # Use title as the keyword\n",
    "        sublist = [titles[i]]  \n",
    "        prob_sublist = [0.99]\n",
    "    else:\n",
    "        # Ensure new_prob does not exceed 1\n",
    "        new_prob = min(prob_sublist[0] + 0.1, 1.0)  \n",
    "        if titles[i] in sublist:\n",
    "            # Remove duplicate keyword and corresponding probability\n",
    "            index = sublist.index(titles[i])\n",
    "            del sublist[index]  \n",
    "            del prob_sublist[index]\n",
    "        # Add title to head of keywords\n",
    "        sublist = [titles[i]] + sublist  \n",
    "        prob_sublist = [new_prob] + prob_sublist\n",
    "    \n",
    "    updated_keywords_list.append(sublist)\n",
    "    updated_prob_list.append(prob_sublist)\n",
    "\n",
    "assert len(updated_prob_list) == len(updated_keywords_list)\n",
    "print(f\"Total num: {len(updated_keywords_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {   \n",
    "        \"id\": id, \n",
    "        \"context id\": c_id,\n",
    "        \"keywords\": kw,\n",
    "        \"prob\": p,\n",
    "    } \n",
    "    for id, c_id, kw, p in zip(ids, context_ids, updated_keywords_list, updated_prob_list)\n",
    "]\n",
    "\n",
    "json_data = json.dumps(documents, indent=4)\n",
    "\n",
    "with open('/kaggle/working/Query_BART_Keywords_with_prob_and_title_MLQA_en.json', 'w') as f:\n",
    "    f.write(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 11590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'a4968ca8a18de16aa3859be760e43dbd3af3fce9',\n",
       "  'context id': 0,\n",
       "  'keywords': ['Area 51', 'Bi', 'Biopsies'],\n",
       "  'prob': [0.90947, 0.80947, 0.553737]},\n",
       " {'id': 'f251ea56c4f1aa1df270137f7e6d89c0cc1b6ef4',\n",
       "  'context id': 0,\n",
       "  'keywords': ['Area 51', 'RobertKasza', 'Frost', 'Lawsuit'],\n",
       "  'prob': [0.773948, 0.673948, 0.663587, 0.753613]},\n",
       " {'id': '04ecd5555635bc05fd2f379d1b9027edd663cebf',\n",
       "  'context id': 0,\n",
       "  'keywords': ['Area 51', 'Groom', 'Lawsuit'],\n",
       "  'prob': [0.524797, 0.424797, 0.579578]},\n",
       " {'id': 'd066a75dbe8cd3e2b57c415a8eb54a08dc7e72a7',\n",
       "  'context id': 0,\n",
       "  'keywords': ['Area 51', 'Complaintants', 'alleged'],\n",
       "  'prob': [0.739551, 0.639551, 0.296297]},\n",
       " {'id': 'c5f545baccd8ea8adb83f75756f4832340600bd9',\n",
       "  'context id': 1,\n",
       "  'keywords': ['Area 51', 'A Aerospace Magazine'],\n",
       "  'prob': [0.699785, 0.599785]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path_ = '/kaggle/working/Query_BART_Keywords_with_prob_and_title_MLQA_en.json'\n",
    "\n",
    "with open(file_path_, 'r') as f:\n",
    "    data_ = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(data_)}\")\n",
    "data_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Context **Add `title` to `query`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 9916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'title': 'Area 51',\n",
       "  'context': 'In 1994, five unnamed civilian contractors and the widows of contractors Walter Kasza and Robert Frost sued the USAF and the United States Environmental Protection Agency. Their suit, in which they were represented by George Washington University law professor Jonathan Turley, alleged they had been present when large quantities of unknown chemicals had been burned in open pits and trenches at Groom. Biopsies taken from the complainants were analyzed by Rutgers University biochemists, who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat. The complainants alleged they had sustained skin, liver, and respiratory injuries due to their work at Groom, and that this had contributed to the deaths of Frost and Kasza. The suit sought compensation for the injuries they had sustained, claiming the USAF had illegally handled toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs handling of dangerous materials). They also sought detailed information about the chemicals to which they were allegedly exposed, hoping this would facilitate the medical treatment of survivors. Congressman Lee H. Hamilton, former chairman of the House Intelligence Committee, told 60 Minutes reporter Lesley Stahl, \"The Air Force is classifying all information about Area 51 in order to protect themselves from a lawsuit.\"'},\n",
       " {'id': 1,\n",
       "  'title': 'Area 51',\n",
       "  'context': 'In January 2006, space historian Dwayne A. Day published an article in online aerospace magazine The Space Review titled \"Astronauts and Area 51: the Skylab Incident\". The article was based on a memo written in 1974 to CIA director William Colby by an unknown CIA official. The memo reported that astronauts on board Skylab 4 had, as part of a larger program, inadvertently photographed a location of which the memo said:'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = './data/Context_EN.json'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    data_en = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(data_en)}\")\n",
    "data_en[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 9916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'title': 'Area 51',\n",
       "  'keywords': ['Civilian Contractors',\n",
       "   'USAF',\n",
       "   'Environmental Protection Agency',\n",
       "   'Area 51',\n",
       "   'Toxic Materials']},\n",
       " {'id': 1,\n",
       "  'title': 'Area 51',\n",
       "  'keywords': ['Astronauts', 'Area 51', 'Skylab Incident']},\n",
       " {'id': 2,\n",
       "  'title': 'Area 51',\n",
       "  'keywords': ['Area 51', 'UFO', 'Conspiracy Theories']}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_ = './data/MLQA_Keywords/en/Context_with_title_BART_Keywords_MLQA_en.json'\n",
    "\n",
    "with open(file_path_, 'r') as f:\n",
    "    data_ = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(data_)}\")\n",
    "data_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 9916\n",
      "Total num: 9916\n"
     ]
    }
   ],
   "source": [
    "ids = [item['id'] for item in data_]\n",
    "kws = [item['keywords'] for item in data_]\n",
    "titles = [item['title'] for item in data_]\n",
    "titles = [re.sub(r'[^\\w\\s]+', '', title) for title in titles]\n",
    "\n",
    "assert len(ids) == len(titles)\n",
    "assert len(ids) == len(kws)\n",
    "\n",
    "\n",
    "strip_key_words = [([titles[i]] + sublist) for i, sublist in enumerate(kws)]\n",
    "\n",
    "assert len(ids) == len(strip_key_words)\n",
    "\n",
    "print(f\"Total num: {len(strip_key_words)}\")\n",
    "\n",
    "unique_data = [list(dict.fromkeys(sublist)) for sublist in strip_key_words]\n",
    "\n",
    "assert len(unique_data) == len(strip_key_words)\n",
    "print(f\"Total num: {len(unique_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {   \n",
    "        \"id\": id, \n",
    "        \"keywords\": kw\n",
    "    } \n",
    "    for id, kw in zip(ids, unique_data)\n",
    "]\n",
    "\n",
    "json_data = json.dumps(documents, indent=4)\n",
    "\n",
    "with open('./data/MLQA_Keywords/en/Context_with_title_BART_Keywords_MLQA_en.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 9916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'keywords': ['Area 51',\n",
       "   'Civilian Contractors',\n",
       "   'USAF',\n",
       "   'Environmental Protection Agency',\n",
       "   'Toxic Materials']},\n",
       " {'id': 1, 'keywords': ['Area 51', 'Astronauts', 'Skylab Incident']},\n",
       " {'id': 2, 'keywords': ['Area 51', 'UFO', 'Conspiracy Theories']}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_ = './data/MLQA_Keywords/en/Context_with_title_BART_Keywords_MLQA_en.json'\n",
    "\n",
    "with open(file_path_, 'r') as f:\n",
    "    data_ = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(data_)}\")\n",
    "data_[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
