{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2Model\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Version 1: Use DeBERTa and Tf-Idf to calculate document and word similarity\n",
    "\n",
    "Calculate the similarity between the context output and the keyword output of tf-idf, also give lower weight to common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertKeywordExtractor:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "        self.model = DebertaV2Model.from_pretrained('microsoft/deberta-v3-base')\n",
    "\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def refine_keywords(self, text, candidate_keywords, num_keywords):\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "        doc_embedding = embeddings[:,0,:]\n",
    "\n",
    "        refined_keywords = {}\n",
    "        for keyword, w in candidate_keywords.items():\n",
    "            kw_inputs = self.tokenizer(keyword, return_tensors='pt', padding=True, truncation=True)\n",
    "            kw_inputs = {k: v.to(self.device) for k, v in kw_inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                kw_outputs = self.model(**kw_inputs)\n",
    "            kw_embedding = kw_outputs.last_hidden_state[:,0,:]\n",
    "\n",
    "            similarity = torch.cosine_similarity(doc_embedding, kw_embedding)\n",
    "            if similarity > 0.15:  # 阈值可以调整\n",
    "                refined_keywords[keyword] = similarity * w\n",
    "            \n",
    "        sorted_kw = list(dict(sorted(refined_keywords.items(), key = lambda item: item[1], reverse=True)))\n",
    "\n",
    "        return sorted_kw[:num_keywords]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use TfidfVectorizer to extract keywords\n",
    "Generate scores for unigram and bigrams for each document, and filter out invalid collocations and redundant words. The weights of words with different frequencies are inversely normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ruzexi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def pos_tag(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    pos_dict = {token.text: token.pos_ for token in doc}\n",
    "    return pos_dict\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def filter_bigrams(text, bigrams):\n",
    "    pos_dic=pos_tag(text[0])\n",
    "    sentences = sent_tokenize(text[0])\n",
    "    # valid_unigrams = []\n",
    "    valid_bigrams = {}\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        #unigram\n",
    "        if ' ' not in bigram and not re.match(r'\\d{1,3}(,\\d{3})*', bigram) and \\\n",
    "            (bigram not in pos_dic or pos_dic[bigram] in ('NOUN', 'PROPN', 'ADJ')):\n",
    "                valid_bigrams[bigram] = bigrams[bigram] * 1.05\n",
    "                \n",
    "        elif ' ' in bigram and any(bigram in sentence for sentence in sentences):\n",
    "            bigram_token=bigram.split()\n",
    "            if bigram_token[0] not in pos_dic or bigram_token[1] not in pos_dic:\n",
    "                if '-' in bigram_token[0]:\n",
    "                    valid_bigrams[bigram] = bigrams[bigram]\n",
    "            elif pos_dic[bigram_token[0]] in ('NOUN', 'PROPN') and pos_dic[bigram_token[1]] in ('NOUN', 'PROPN') or \\\n",
    "               pos_dic[bigram_token[0]] == 'ADJ' and pos_dic[bigram_token[1]] in ('NOUN', 'PROPN'):\n",
    "                   valid_bigrams[bigram] = bigrams[bigram]\n",
    "            \n",
    "    \n",
    "    return valid_bigrams\n",
    "\n",
    "\n",
    "class TFIDFKeywordExtractor:\n",
    "    def __init__(self, index, documents):\n",
    "        self.documents = documents\n",
    "        self.index = index\n",
    "        # self.text = documents[index]\n",
    "        vectorizer = TfidfVectorizer(lowercase=False, stop_words='english', ngram_range=(1,2),token_pattern=r\"(?u)\\b\\w+[-\\w]+\\b\")\n",
    "        vectorizer.fit([self.documents[self.index]])\n",
    "        vocab = vectorizer.vocabulary_\n",
    "        self.vectorizer = TfidfVectorizer(lowercase=False, stop_words='english', ngram_range=(1,2),vocabulary=vocab,token_pattern=r\"(?u)\\b\\w+[-\\w]+\\b\")\n",
    "        \n",
    "        \n",
    "\n",
    "    def extract_keywords(self):\n",
    "        \n",
    "        tfidf_matrix = self.vectorizer.fit_transform(self.documents)\n",
    "        importance = np.array(np.sum(tfidf_matrix, axis=0).flatten())[0]\n",
    "        normalized_tfidf = softmax(1 - importance / np.max(importance))\n",
    "        # print(importance)\n",
    "        feature_array = self.vectorizer.get_feature_names_out()\n",
    "        # feature_array\n",
    "        tfidf_sorting = normalized_tfidf.argsort()[::-1]\n",
    "        \n",
    "        \n",
    "        keywords = feature_array[tfidf_sorting]\n",
    "        weight = sorted(normalized_tfidf)[::-1]\n",
    "        kw_dic = dict(zip(keywords,weight))\n",
    "     \n",
    "        valid_keywords = filter_bigrams(self.documents, kw_dic)\n",
    "        valid_keywords = self.remove_redundant_keywords(valid_keywords)\n",
    "        # print(f'here is my{valid_keywords}')\n",
    "        # print()\n",
    "        \n",
    "        return valid_keywords\n",
    "\n",
    "    def remove_redundant_keywords(self, keywords):\n",
    "        refined_keywords = {}\n",
    "        for kw in keywords:\n",
    "            if len(kw.split()) == 1:\n",
    "                if not any(kw in multi_kw for multi_kw in keywords if len(multi_kw.split()) > 1):\n",
    "                    refined_keywords[kw] = keywords[kw]\n",
    "            # elif kw not in list(ENGLISH_STOP_WORDS):\n",
    "            else:\n",
    "                refined_keywords[kw] = keywords[kw]\n",
    "                \n",
    "        return refined_keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "Normalisation, Lemmatisation, Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ruzexi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  0%|          | 0/30042 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  0%|          | 1/30042 [00:19<166:17:53, 19.93s/it]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  0%|          | 2/30042 [00:37<153:34:18, 18.40s/it]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  0%|          | 2/30042 [00:42<177:05:36, 21.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb 单元格 7\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m                 keywords \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m extract_keywords_for_text(i, texts, num_keywords)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     main()\n",
      "\u001b[1;32m/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb 单元格 7\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     num_keywords \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(texts[i])\u001b[39m/\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     keywords \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m extract_keywords_for_text(i, texts, num_keywords)\n",
      "\u001b[1;32m/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb 单元格 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m candidate_keywords \u001b[39m=\u001b[39m tfidf_extractor\u001b[39m.\u001b[39mextract_keywords()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m bert_extractor \u001b[39m=\u001b[39m BertKeywordExtractor()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m refined_keywords \u001b[39m=\u001b[39m bert_extractor\u001b[39m.\u001b[39;49mrefine_keywords(text[index], candidate_keywords, num_keywords)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m lemmatized_keywords \u001b[39m=\u001b[39m lemmatize_keywords(refined_keywords)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m lemmatized_keywords\n",
      "\u001b[1;32m/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb 单元格 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m kw_inputs \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kw_inputs\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     kw_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw_inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m kw_embedding \u001b[39m=\u001b[39m kw_outputs\u001b[39m.\u001b[39mlast_hidden_state[:,\u001b[39m0\u001b[39m,:]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ruzexi/inf301/comp0087/keyword_extractor.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m similarity \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcosine_similarity(doc_embedding, kw_embedding)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1070\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m   1062\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1063\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1064\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1068\u001b[0m )\n\u001b[0;32m-> 1070\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1071\u001b[0m     embedding_output,\n\u001b[1;32m   1072\u001b[0m     attention_mask,\n\u001b[1;32m   1073\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1074\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1075\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1076\u001b[0m )\n\u001b[1;32m   1077\u001b[0m encoded_layers \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1079\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:514\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    504\u001b[0m     output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    505\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    506\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         output_attentions,\n\u001b[1;32m    512\u001b[0m     )\n\u001b[1;32m    513\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 514\u001b[0m     output_states \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    515\u001b[0m         next_kv,\n\u001b[1;32m    516\u001b[0m         attention_mask,\n\u001b[1;32m    517\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    518\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    519\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    520\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    524\u001b[0m     output_states, att_m \u001b[39m=\u001b[39m output_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:362\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    354\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    355\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m ):\n\u001b[0;32m--> 362\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    363\u001b[0m         hidden_states,\n\u001b[1;32m    364\u001b[0m         attention_mask,\n\u001b[1;32m    365\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    366\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    367\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    368\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[1;32m    370\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    371\u001b[0m         attention_output, att_matrix \u001b[39m=\u001b[39m attention_output\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:293\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    285\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    286\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m     rel_embeddings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    292\u001b[0m ):\n\u001b[0;32m--> 293\u001b[0m     self_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    294\u001b[0m         hidden_states,\n\u001b[1;32m    295\u001b[0m         attention_mask,\n\u001b[1;32m    296\u001b[0m         output_attentions,\n\u001b[1;32m    297\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    298\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    299\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    302\u001b[0m         self_output, att_matrix \u001b[39m=\u001b[39m self_output\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:721\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelative_attention:\n\u001b[1;32m    720\u001b[0m     rel_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[0;32m--> 721\u001b[0m     rel_att \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisentangled_attention_bias(\n\u001b[1;32m    722\u001b[0m         query_layer, key_layer, relative_pos, rel_embeddings, scale_factor\n\u001b[1;32m    723\u001b[0m     )\n\u001b[1;32m    725\u001b[0m \u001b[39mif\u001b[39;00m rel_att \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m rel_att\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:774\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    771\u001b[0m rel_embeddings \u001b[39m=\u001b[39m rel_embeddings[\u001b[39m0\u001b[39m : att_span \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    772\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_att_key:\n\u001b[1;32m    773\u001b[0m     pos_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\n\u001b[0;32m--> 774\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery_proj(rel_embeddings), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads\n\u001b[1;32m    775\u001b[0m     )\u001b[39m.\u001b[39mrepeat(query_layer\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    776\u001b[0m     pos_key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_proj(rel_embeddings), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads)\u001b[39m.\u001b[39mrepeat(\n\u001b[1;32m    777\u001b[0m         query_layer\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m\n\u001b[1;32m    778\u001b[0m     )\n\u001b[1;32m    779\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_keywords(keywords):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_keywords = set()  # Using a set to avoid duplicates\n",
    "    # print(keywords)\n",
    "    for keyword in keywords:\n",
    "        lemmatized_keyword = lemmatizer.lemmatize(keyword)\n",
    "        lemmatized_keywords.add(lemmatized_keyword)\n",
    "        # print(lemmatized_keywords)\n",
    "    return list(lemmatized_keywords)\n",
    "\n",
    "\n",
    "def extract_keywords_for_text(index, text, num_keywords):\n",
    "    # Assuming TFIDFKeywordExtractor and BertKeywordExtractor are already implemented\n",
    "    tfidf_extractor = TFIDFKeywordExtractor(index, text)\n",
    "    candidate_keywords = tfidf_extractor.extract_keywords()\n",
    "\n",
    "    bert_extractor = BertKeywordExtractor()\n",
    "    refined_keywords = bert_extractor.refine_keywords(text[index], candidate_keywords, num_keywords)\n",
    "    lemmatized_keywords = lemmatize_keywords(refined_keywords)\n",
    "    \n",
    "    return lemmatized_keywords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_json(file_path):\n",
    "    # Function to read the JSON file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def segment_text(id, text, max_length=512, step_size=256):\n",
    "    \"\"\"\n",
    "    After exceeding the maximum bert input limit, use the slide window to truncate to size=256\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        if start + max_length > len(text):\n",
    "            segment = text[start:]\n",
    "            segments.append(segment)\n",
    "            id.append(id[-1]+1)\n",
    "            break\n",
    "        \n",
    "        segment = text[start:start+max_length]\n",
    "        segments.append(segment)\n",
    "        id.append(id[-1])\n",
    "        start += step_size\n",
    "\n",
    "    return segments\n",
    "\n",
    "    \n",
    "def preprocess(file_path):\n",
    "    data = read_json(file_path)\n",
    "    preprocessed_data = []\n",
    "    id_context=[0]\n",
    "    \n",
    "    \n",
    "    for item in data:\n",
    "        context = item['context']\n",
    "        context = re.sub(r'\\s+', ' ', context).strip()\n",
    "        segmented_contexts = segment_text(id_context, context)\n",
    "        preprocessed_data.extend(segmented_contexts) \n",
    "      \n",
    "\n",
    "    return preprocessed_data, id_context\n",
    "\n",
    "def main():\n",
    "    FILE_PATH = \"Extract.json\"\n",
    "    texts, id_context = preprocess(FILE_PATH)\n",
    "    \n",
    "    with open('new.txt', 'w') as file:\n",
    "        num_keywords = int(len(texts[0])/30)\n",
    "        keywords = extract_keywords_for_text(0, texts, num_keywords)\n",
    "        for i in tqdm(range(1, len(texts))):\n",
    "            if id_context[i] != id_context[i-1]:\n",
    "                file.write(f\"Text {id_context[i]-1} Keywords: {', '.join(list(set(keywords)))}\\n\\n\")\n",
    "                keywords = extract_keywords_for_text(i, texts, num_keywords)\n",
    "            else:\n",
    "                num_keywords = int(len(texts[i])/30)\n",
    "                keywords += extract_keywords_for_text(i, texts, num_keywords)\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2, using a pre-trained model dedicated to keyword extraction: bert-uncased-keyword-extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanekyuk/bert-uncased-keyword-extractor\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"yanekyuk/bert-uncased-keyword-extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kw_extractor = pipeline(\"token-classification\", \n",
    "                        model=\"yanekyuk/bert-uncased-keyword-extractor\",\n",
    "                        tokenizer = tokenizer)\n",
    "\n",
    "def KeyWords_generator(outputs, text):\n",
    "    keywords = []\n",
    "    current_keyword = None\n",
    "    current_start = None\n",
    "    current_end = None\n",
    "\n",
    "    for item in outputs:\n",
    "        if item['entity'] == 'B-KEY':\n",
    "            if current_keyword:\n",
    "                keyword = text[current_start:current_end]\n",
    "                if keyword.count(' ') < 4:\n",
    "                    keywords.append(keyword) \n",
    "                current_keyword = None\n",
    "            current_start = item['start']\n",
    "            current_keyword = item['word']\n",
    "        elif item['entity'] == 'I-KEY' and current_keyword:\n",
    "            current_keyword = item['word']\n",
    "        current_end = item['end']\n",
    "\n",
    "\n",
    "    if current_keyword:\n",
    "        keyword = text[current_start:current_end]\n",
    "        if keyword.count(' ') < 4:\n",
    "            keywords.append(keyword)\n",
    "\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30042 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anonymous', 'editor-in-chief', 'Julian Assange', 'WikiLeaks', 'Sunshine Press', 'Kristinn Hrafnsson', 'Iceland', 'Internet']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/30042 [00:01<3:34:17,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Soviet Union', 'Hiroshima', 'Nagasaki', 'Japan', 'war crimes', 'unconditional surrender', 'Adolf Hitler', 'atomic bombs', 'Potsdam Declaration', 'Western Allies', 'Soviet', 'United States']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/30042 [00:02<3:17:44,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marriage equality', 'Loving v. Virginia', 'Due Process Clause', 'gay marriage', 'separate marriage', 'Same-sex marriage']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/30042 [00:03<4:17:08,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = \"Extract.json\"\n",
    "texts, id_context = preprocess(FILE_PATH)\n",
    "FILE_WRITE = \"Keywords.json\"\n",
    "unpreprocess_data = read_json(FILE_PATH)\n",
    "\n",
    "\n",
    "def main():\n",
    "    doc_kw = []\n",
    "    with open(FILE_WRITE, 'w') as file:\n",
    "        keywords = KeyWords_generator(kw_extractor(texts[0]), texts[0])\n",
    "        for i in tqdm(range(1, len(texts))):\n",
    "            if id_context[i] != id_context[i-1]:\n",
    "                kw_list = list(set(keywords))\n",
    "                print(kw_list)\n",
    "                data_dic = {\n",
    "                    'context': unpreprocess_data[id_context[i] - 1],\n",
    "                    'keywords': kw_list\n",
    "                }\n",
    "                # file.write(f\"Text {id_context[i]-1} Keywords: {', '.join(list(set(keywords)))}\\n\\n\")\n",
    "                doc_kw.append(data_dic)\n",
    "                keywords = KeyWords_generator(kw_extractor(texts[i]), texts[i])\n",
    "            else:\n",
    "                keywords += KeyWords_generator(kw_extractor(texts[i]), texts[i])\n",
    "        json.dump(doc_kw, file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "       \n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
