{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Keywords (Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForTokenClassification\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 14656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'In what year did WikiLeaks first display information on the Internet?',\n",
       "  'answer': '2006',\n",
       "  'context': 'WikiLeaks () is an international non-profit organisation that publishes secret information, news leaks, and classified media provided by anonymous sources. Its website, initiated in 2006 in Iceland by the organisation Sunshine Press, claims a database of 10 million documents in 10 years since its launch. Julian Assange, an Australian Internet activist, is generally described as its founder and director. Kristinn Hrafnsson is its editor-in-chief.',\n",
       "  'context_id': 0},\n",
       " {'question': 'Which country was defeated in the Second World War?',\n",
       "  'answer': 'Germany',\n",
       "  'context': 'The war in Europe concluded with an invasion of Germany by the Western Allies and the Soviet Union, culminating in the capture of Berlin by Soviet troops, the suicide of Adolf Hitler and the German unconditional surrender on 8 May 1945. Following the Potsdam Declaration by the Allies on 26 July 1945 and the refusal of Japan to surrender under its terms, the United States dropped atomic bombs on the Japanese cities of Hiroshima and Nagasaki on 6 and 9 August respectively. With an invasion of the Japanese archipelago imminent, the possibility of additional atomic bombings, the Soviet entry into the war against Japan and its invasion of Manchuria, Japan announced its intention to surrender on 15 August 1945, cementing total victory in Asia for the Allies. Tribunals were set up by fiat by the Allies and war crimes trials were conducted in the wake of the war both against the Germans and the Japanese.',\n",
       "  'context_id': 1},\n",
       " {'question': 'How many Arab soldiers died in the Arab-Israeli war?',\n",
       "  'answer': 'unknown',\n",
       "  'context': \"The exact number of Arab casualties is unknown. One estimate places the Arab death toll at 7,000, including 3,000 Palestinians, 2,000 Egyptians, 1,000 Jordanians, and 1,000 Syrians. In 1958, Palestinian historian Aref al-Aref calculated that the Arab armies' combined losses amounted to 3,700, with Egypt losing 961 regular and 200 irregular soldiers and Jordan losing 362 regulars and 200 irregulars. According to Henry Laurens, the Palestinians suffered double the Jewish losses, with 13,000 dead, 1,953 of whom are known to have died in combat situations. Of the remainder, 4,004 remain nameless but the place, tally and date of their death is known, and a further 7,043, for whom only the place of death is known, not their identities nor the date of their death. According to Laurens, the largest part of Palestinian casualties consisted of non-combatants and corresponds to the successful operations of the Israelis.\",\n",
       "  'context_id': 2},\n",
       " {'question': \"When was the world's first capitalist society formed?\",\n",
       "  'answer': '17th century',\n",
       "  'context': 'As Thomas Hall (2000) notes, \"The Sung Empire in particular nearly underwent a transformation to capitalism in the tenth century C.E. But the only states to be controlled by capitalists before the European transformation in the seventeenth century were semiperipheral capitalist city-states such as the Phoenician cities, Venice, Genoa, and Malacca. These operated in the interstices between the tributary states and empires, and though they were agents of commodification, they existed within larger systems in which the logic of state-based coercion remained dominant. The first capitalist nation-state was the Dutch Republic in the seventeenth century. This coming to state power by capitalists in an emerging core region signaled the triumph of regional capitalism in the European subsystem.\" In Werner Sombart\\'s words, \"In all probability the United Provinces were the land in which the capitalist spirit for the first time attained its fullest maturity; where this maturity related to all its aspects, which were equally developed; and where this development had never been done comprehensive before. Moreover, in the Netherlands an entire people became imbued with the capitalist spirit; so much so, that in the 17th century Holland was universally regarded as the land of capitalism par exellence; it was envied by all other nations, who put forth their keenest endeavours in their desire to emulate it; it was the high school of every art of the tradesman, and the well-watered garden wherein the middle-class virtues throve.\" Foundational thinkers for this approach include Adam Smith, Max Weber, Fernand Braudel, Henri Pirenne, and Paul Sweezy.',\n",
       "  'context_id': 3},\n",
       " {'question': 'When did the First World War officially start?',\n",
       "  'answer': '28 July 1914',\n",
       "  'context': 'World War I (often abbreviated as WWI or WW1), also known as the First World War or the Great War, was a global war originating in Europe that lasted from 28 July 1914 to 11 November 1918. Contemporaneously described as \"the war to end all wars\", it led to the mobilisation of more than 70 million military personnel, including 60 million Europeans, making it one of the largest wars in history. It is also one of the deadliest conflicts in history, with an estimated nine million combatants and seven million civilian deaths as a direct result of the war, while resulting genocides and the 1918 influenza pandemic caused another 50 to 100 million deaths worldwide.',\n",
       "  'context_id': 4}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = './data/context_qa_en.json'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(data)}\")\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up BART**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecc3819a3bc425a84b70cd6df7c5e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee9410abe0e437e914127366b47cf46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282b5b39b081410cb3e766b5edc6bdaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc21b97387a48b489072b3f8315d955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8e27da2eb24932a37b403023052359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19334cea279c4d68bde61fc6c13e7080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f323aa97dc433c8e5ce17d01477214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242226b93668480197b89347a4a997ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/292 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Andyrasika/bart_tech_keywords\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Andyrasika/bart_tech_keywords\")\n",
    "\n",
    "'''\n",
    "    * For \"Question\", set `texts` with `[item['question'] for item in data]`\n",
    "    * For \"Context\", set `texts` with `[item['context'] for item in data]`\n",
    "'''\n",
    "texts = [item['question'] for item in data]\n",
    "\n",
    "encodings = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Use dataloader\n",
    "dataset = TextDataset(encodings)\n",
    "dataloader = DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")\n",
    "text_outputs = []\n",
    "\n",
    "for batch in tqdm(dataloader, desc=\"Generating text\"):\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()} \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**batch)\n",
    "        decoded_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        text_outputs.extend(decoded_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(texts) == len(text_outputs)\n",
    "len(texts), len(text_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_key_words = [line.strip() for line in text_outputs]\n",
    "\n",
    "'''\n",
    "    * For \"Question\", set `ids` with `item['id']`\n",
    "    * For \"Context\", set `ids` with `item['context_id']`\n",
    "'''\n",
    "ids = [item['id'] for item in data]\n",
    "\n",
    "assert len(ids) == len(strip_key_words)\n",
    "\n",
    "print(f\"Total num: {len(strip_key_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {   \n",
    "        \"id\": id, \n",
    "        \"keywords\": [k.strip() for k in kw.split(',')]\n",
    "    } \n",
    "    for id, kw in zip(ids, strip_key_words)\n",
    "]\n",
    "\n",
    "json_data = json.dumps(documents, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/kaggle/working/Query_BART_Keywords.json', 'w') as f:\n",
    "    f.write(json_data)\n",
    "\n",
    "# with open('./data/BART_Keywords.json', 'w') as f:\n",
    "#     f.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial add prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 14656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'context_id': 0, 'keywords': ['WikiLeaks', 'Information Displaying']},\n",
       " {'context_id': 1, 'keywords': ['Second World War', 'Defeat']},\n",
       " {'context_id': 2, 'keywords': ['Arab-Israeli War', 'Deaths']},\n",
       " {'context_id': 3, 'keywords': ['Capitalist Society']},\n",
       " {'context_id': 4, 'keywords': ['First World War']}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_ = '/kaggle/working/Query_BART_Keywords.json'\n",
    "\n",
    "with open(file_path_, 'r') as f:\n",
    "    Query_BART_Keywords = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(Query_BART_Keywords)}\")\n",
    "Query_BART_Keywords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context_id': 0,\n",
       "  'keywords': ['WikiLeaks', 'Information Displaying'],\n",
       "  'prob': [0.9, 0.8]},\n",
       " {'context_id': 1,\n",
       "  'keywords': ['Second World War', 'Defeat'],\n",
       "  'prob': [0.9, 0.8]},\n",
       " {'context_id': 2,\n",
       "  'keywords': ['Arab-Israeli War', 'Deaths'],\n",
       "  'prob': [0.9, 0.8]},\n",
       " {'context_id': 3, 'keywords': ['Capitalist Society'], 'prob': [0.9]},\n",
       " {'context_id': 4, 'keywords': ['First World War'], 'prob': [0.9]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for item in Query_BART_Keywords:\n",
    "    num_keywords = len(item[\"keywords\"])\n",
    "    probs = [max(0.9 - 0.1 * i, 0.1) for i in range(num_keywords)]\n",
    "    item[\"prob\"] = probs\n",
    "\n",
    "Query_BART_Keywords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.dumps(Query_BART_Keywords, indent=4)\n",
    "\n",
    "with open('/kaggle/working/Query_BART_Keywords_artificially_prob.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bart generate prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, encodings):\n",
    "#         self.encodings = encodings\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.encodings.input_ids)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Andyrasika/bart_tech_keywords\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"Andyrasika/bart_tech_keywords\")\n",
    "\n",
    "# texts = [item['question'] for item in data]\n",
    "\n",
    "# encodings = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# # Use dataloader\n",
    "# dataset = TextDataset(encodings)\n",
    "# dataloader = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set model to evaluate mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3700fda6f104332b7a93ec1d06b3507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34/4251668379.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "all_input_texts = []\n",
    "all_generated_texts = []\n",
    "all_probabilities = []\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    input_ids = batch['input_ids'].to(model.device)\n",
    "    attention_mask = batch['attention_mask'].to(model.device)\n",
    "\n",
    "    for idx in range(input_ids.size(0)):  # Iter all in a batch\n",
    "        decoded_ids = torch.full((1, 1), tokenizer.bos_token_id, dtype=torch.long, device=model.device)\n",
    "        seq_probabilities = []\n",
    "\n",
    "        input_text = tokenizer.decode(input_ids[idx].tolist(), skip_special_tokens=True)\n",
    "        all_input_texts.append(input_text) \n",
    "        \n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids[idx:idx+1], attention_mask=attention_mask[idx:idx+1], decoder_input_ids=decoded_ids)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "                probs = softmax(logits, dim=-1)\n",
    "                next_token_id = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "                next_token_prob = probs[0, next_token_id.item()].item()\n",
    "\n",
    "                if next_token_id == tokenizer.eos_token_id or decoded_ids.size(1) >= 30:\n",
    "                    break\n",
    "\n",
    "                decoded_ids = torch.cat([decoded_ids, next_token_id], dim=-1)\n",
    "                seq_probabilities.append(next_token_prob)\n",
    "\n",
    "        generated_text = tokenizer.decode(decoded_ids[:, 1:].squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "        # Splitting the generated text and probability\n",
    "        segments = generated_text.split(',')\n",
    "        segment_probs = []\n",
    "        start_idx = 0\n",
    "        for segment in segments:\n",
    "            segment_tokens = tokenizer.tokenize(segment.strip())\n",
    "            segment_length = len(segment_tokens)\n",
    "            # Calc avg prob of the segment\n",
    "            if segment_length > 0:\n",
    "                segment_prob = sum(seq_probabilities[start_idx:start_idx+segment_length]) / segment_length\n",
    "                segment_probs.append(f\"{segment_prob:.6f}\")\n",
    "            start_idx += segment_length\n",
    "\n",
    "        all_generated_texts.append(generated_text)\n",
    "        all_probabilities.append(\",\".join(segment_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14656, 14656)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "strip_key_words = [line.strip() for line in all_generated_texts]\n",
    "\n",
    "'''\n",
    "    * For \"Question\", set `ids` with `[item['id'] for item in data]`\n",
    "    * For \"Context\", set `ids` with `[item['context_id'] for item in data]`\n",
    "'''\n",
    "ids = [item['id'] for item in data]\n",
    "\n",
    "len(ids),len(strip_key_words)\n",
    "\n",
    "# print(f\"Total num: {len(strip_key_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = [\n",
    "#     {\"context_id\": id, \"keywords\": [k.strip() for k in kw.split(',')], \"prob\": [k.strip() for k in p.split(',')]} \n",
    "#     for id, kw, p in zip(ids, strip_key_words, all_probabilities)\n",
    "# ]\n",
    "\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": id, \n",
    "        \"keywords\": [k.strip() for k in kw.split(',')],\n",
    "        \"prob\": [float(p) for p in prob.split(',')]\n",
    "    } \n",
    "    for id, kw, prob in zip(ids, strip_key_words, all_probabilities)\n",
    "    if kw.strip() != '' and prob.strip() != ''\n",
    "]\n",
    "\n",
    "json_data = json.dumps(documents, indent=4)\n",
    "# print(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/kaggle/working/Query_BART_Keywords_with_prob.json', 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num: 14439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': -2745676532874802308,\n",
       "  'keywords': ['WikiLeaks', 'Internet'],\n",
       "  'prob': [0.938878, 0.677631]},\n",
       " {'id': -1876566410524927695,\n",
       "  'keywords': ['SecondWorld War II'],\n",
       "  'prob': [0.774945]},\n",
       " {'id': 4765871946129153693,\n",
       "  'keywords': ['ArabIsraeli War', 'Arab Soldiers'],\n",
       "  'prob': [0.779647, 0.733196]},\n",
       " {'id': -420328556017322612,\n",
       "  'keywords': ['CapitalCapitalist Society'],\n",
       "  'prob': [0.905467]},\n",
       " {'id': -4747113494541566005,\n",
       "  'keywords': ['FirstWorld War I'],\n",
       "  'prob': [0.709858]}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path_ = '/kaggle/working/BART_Keywords_with_prob.json'\n",
    "\n",
    "with open(file_path_, 'r') as f:\n",
    "    data_ = json.load(f)\n",
    "\n",
    "print(f\"Total num: {len(data_)}\")\n",
    "data_[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check empty output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "empty_keywords_context_ids = [entry['context_id'] for entry in data_ if not entry['keywords']]\n",
    "\n",
    "print(empty_keywords_context_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "empty_id = [d['id'] for d in data_ if d['keywords']==['']]\n",
    "len(empty_id), empty_id[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
